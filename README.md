# Deep_Neural_Networks_with_PyTorch

By IBM on Coursera

--------------------------------------------------------------------------------------------------------------------------------------------------------

1. Week 1: **Tensor and Datasets** </br>
                > Lab Tensors 1D _(TorchTensorsIn1D.ipynb)_ </br>
                > Lab Two-Dimensional Tensors _(Two-DimensionalTensors.ipynb)_ </br>
                > Lab Differentiation in PyTorch _(DifferentiationInPyTorch.ipynb)_ </br>
                > Lab Simple Data Set _(SimpleDataset.ipynb)_ </br>
                > Lab Data Set _(ImageDatasetsAndTransforms.ipynb)_ </br>
                > Lab Torch Vision Datasets _(PrebuiltDatasetsAndTransforms.ipynb)_ </br>

2. Week 2: **Linear Regression** </br>
                > Lab Linear Regression 1D: Prediction _(LinearRegression1D:Prediction.ipynb)_ </br>
                > Lab Linear Regression 1D: Prediction _(LinearRegression1D:TrainingOneParameter.ipynb)_ </br>
                > Lab Linear Regression: Prediction _(LinearRegression1D:TrainingTwoParameter.ipynb)_ </br>
                > Lab Linear Regression 1D: PredictionStochastic Gradient Descent and the Data Loader _(LinearRegression1D:TrainingTwoParameterStochasticGradientDescent(SGD).ipynb)_ </br>
                > Mini-Batch Gradient Descent _(LinearRegression1D:TrainingTwoParameterMini-BatchGradientDecent.ipynb)_ </br>
                > Linear Regression 1D PyTroch _(LinearRegression1D:TrainingTwoParameterMini-BatchGradientDescent.ipynb)_ </br>
                > Training, Validation and Test Split in PyTorch _(LinearRegression:TrainingAndValidationData.ipynb)_ </br>

3. Week 3: **Multiple Input Output Linear Regression - Logistic Regression for Classification** </br>
                > Multiple Linear Regression Prediction _(MultipleLinearRegression.ipynb)_ </br>
                > Multiple Linear Regression Training _(LinearRegressionMultipleOutputs1.ipynb)_ </br>
                > Multiple Linear Regression Prediction _(LinearRegressionWithMultipleOutputs.ipynb)_ </br>
                > Multiple Linear Regression Training _(LinearRegressionMultipleOutputs1.ipynb)_ </br>
                > Logistic Regression Prediction _(LogisticRegression.ipynb)_ </br>
                > Logistic Regression Mean Square Error _(LogisticRegressionAndBadInitializationValue.ipynb)_ </br>
                > Logistic Regression Cross Entropy _(LogisticRegressionTrainingNegativeLogLikelihood(Cross-Entropy).ipynb)_ </br>

4. Week 4: **Softmax Regression - Shallow Neural Networks** </br>
                > Softmax Classifier 1 _(SoftmaxClassifier1D.ipynb)_ </br>
                > Softmax Classifier 2 _(SoftmaxClassifier.ipynb)_ </br>
                > Neural Networks in One Dimension _(SimpleOneHiddenLayerNeuralNetwork.ipynb)_ </br>
                > More Hidden Neurons _(NeuralNetworksMoreHiddenNeurons.ipynb)_ </br>
                > Multidimensional Neural Network _(NeuralNetworksWithOneHiddenLayer:NoisyXOR.ipynb)_ </br>
                > Multi-Class Neural Networks with MNIST _(NeuralNetworksWithOneHiddenLayer.ipynb)_ </br>
                > Activation Functions _(ActivationFunctions.ipynb)_ </br>
                > Neural Network with Different Activation Functions _(TestSigmoid,TanhAndReluActivationsFunctionsOnTheMNISTDataset.ipynb)_ </br>

5. Week 5: **Deep Networks** </br>
                > Deep Neural Networks _(HiddenLayerDeepNetwork:Sigmoid,TanhAndReluActivationsFunctionsMNISTDataset.ipynb)_ </br>
                > Deeper Neural Networks : nn.ModuleList() _(DeeperNeuralNetworksWith_nn.ModuleList().ipynb)_ </br>
                > Dropout Classification _(UsingDropoutForClassification.ipynb)_ </br>
                > Dropout regression _(UsingDropoutInRegression.ipynb)_ </br>
                > Initialization _(InitializationWithSameWeights.ipynb)_ </br>
                > Initialization Xavier _(TestUniform,DefaultAndXavierUniformInitializationOnMNISTDatasetWithTanhActivation.ipynb)_ </br>
                > Initialization He _(TestUniform,DefaultAndHeInitializationOnMNISTDatasetWithReluActivation.ipynb)_ </br>
                > Momentum with Different Polynomials _(Momentum.ipynb)_ </br>
                > Neural network momentum _(NeuralNetworksWithMomentum.ipynb)_ </br>
                > Batch Normalization _(BatchNormalizationWithTheMNISTDataset.ipynb)_ </br>

6. Week 6: **Convolutional Neural Networks** </br>
                > What's Convolution _(What'sConvolution.ipynb)_
                > Activation function and Maxpooling _(ActivationFunctionAndMaxpooling)_
                > Multiple Input and Output Channels _(MultipleInputAndOutputChannels.ipynb)_
                > Convolutional Neural Network Simple example _(ConvolutionalNeuralNetworkSimpleExample.ipynb)_
                > Convolutional Neural Network MNIST _(ConvolutionalNeuralNetworkWithSmall_Images.ipynb)_
                > Convolutional Neural Networks with Batch Norm _()_
